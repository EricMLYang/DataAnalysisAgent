"""
Fetch Data Test Flow - LangGraph Implementation

åŸºæ–¼ specs/fetch-data-test.flow_spec.yaml ç”Ÿæˆçš„ LangGraph æµç¨‹
ç›®æ¨™ï¼šè¦åŠƒæ•¸æ“šæ’ˆå–èˆ‡æª¢æŸ¥ä»»å‹™

Phases:
1. Understand - æª¢æŸ¥ç¾æœ‰æ•¸æ“šæ–‡ä»¶
2. Fetch - è®€å– data_fetch_profile.json é…ç½®
3. Profile - è¼‰å…¥ä¸¦æª¢æŸ¥ CSV æ•¸æ“š
4. QualityCheck - é©—è­‰æ•¸æ“šå®Œæ•´æ€§
5. Summarize - ç”Ÿæˆæ•¸æ“šæ‘˜è¦å ±å‘Š
"""

import sys
import json
from pathlib import Path
from typing import TypedDict

from langgraph.graph import StateGraph, START, END

# åŠ å…¥å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ° pathï¼Œä»¥ä¾¿ import skills
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT / ".github" / "skills" / "data-fetch" / "scripts"))

try:
    from fetch import load_and_profile, parse_dataset_key
    FETCH_SKILL_AVAILABLE = True
except ImportError:
    FETCH_SKILL_AVAILABLE = False


# =============================================================================
# State Definition
# =============================================================================

class FlowState(TypedDict):
    """Flow ç‹€æ…‹"""
    task: str                          # è¼¸å…¥ï¼šä»»å‹™æè¿°
    available_files: list[str] | None  # Understand éšæ®µç™¼ç¾çš„æª”æ¡ˆ
    config: dict | None                # Fetch éšæ®µè®€å–çš„é…ç½®
    profile: dict | None               # Profile éšæ®µç”¢ç”Ÿçš„æ•¸æ“š profile
    quality_report: dict | None        # QualityCheck éšæ®µç”¢ç”Ÿçš„å“è³ªå ±å‘Š
    summary: str | None                # Summarize éšæ®µç”¢ç”Ÿçš„æ‘˜è¦
    error: str | None                  # éŒ¯èª¤è¨Šæ¯ï¼ˆå¦‚æœ‰ï¼‰


# =============================================================================
# Node Functions
# =============================================================================

def understand_node(state: FlowState) -> dict:
    """
    Phase: Understand - æª¢æŸ¥ç¾æœ‰æ•¸æ“šæ–‡ä»¶
    
    Steps:
    - æª¢æŸ¥ç¾æœ‰æ•¸æ“šæ–‡ä»¶
    
    # ğŸ¢ Databricks Migration Notes:
    # - æ­¤ node å¯å°è£ç‚º UC Function: `uc.data.list_available_datasets() -> list[str]`
    # - Data Source: å°‡å¾ Unity Catalog æŸ¥è©¢ Delta Tables è€Œéæœ¬åœ°æª”æ¡ˆç³»çµ±
    # - Permissions: éœ€è¦ USAGE æ¬Šé™åœ¨ catalog/schema
    # - Trace: ä½¿ç”¨ MLflow logging å–ä»£ print statements
    """
    task = state["task"]
    
    print(f"[Understand] ä»»å‹™: {task}")
    print(f"[Understand] æª¢æŸ¥å¯ç”¨çš„æ•¸æ“šæ–‡ä»¶...")
    
    # æª¢æŸ¥ mock_data ç›®éŒ„
    mock_data_dir = PROJECT_ROOT / "mock_data"
    available_files = []
    
    if mock_data_dir.exists():
        available_files = [f.name for f in mock_data_dir.glob("*.csv")]
        print(f"[Understand] ç™¼ç¾ {len(available_files)} å€‹ CSV æª”æ¡ˆ: {available_files}")
    else:
        print(f"[Understand] mock_data ç›®éŒ„ä¸å­˜åœ¨")
    
    # æª¢æŸ¥æ˜¯å¦æœ‰ data_fetch_profile.json
    profile_path = PROJECT_ROOT / "data_fetch_profile.json"
    if profile_path.exists():
        print(f"[Understand] ç™¼ç¾ç¾æœ‰ profile æª”æ¡ˆ")
    
    return {
        "available_files": available_files,
        "error": None
    }


def fetch_node(state: FlowState) -> dict:
    """
    Phase: Fetch - è®€å– data_fetch_profile.json é…ç½®
    
    Steps:
    - è®€å– data_fetch_profile.json é…ç½®
    
    Failure Modes:
    - ModuleNotFoundError: å˜—è©¦è¼‰å…¥ pandas åˆ†ææ•¸æ“šå¤±æ•—
    
    Recovery:
    - å¾ pandas æ•¸æ“šåˆ†ææ”¹ç‚ºåŸºæ–¼æª”æ¡ˆçš„åŸºæœ¬æª¢æŸ¥ï¼ˆpandas æœªå®‰è£æ™‚ï¼‰
    
    # ğŸ¢ Databricks Migration Notes:
    # - æ­¤ node å¯å°è£ç‚º UC Function: `uc.data.load_config(config_path: str) -> dict`
    # - Data Source: å°‡å¾ DBFS æˆ– Unity Catalog Volumes è®€å–é…ç½®æª”
    # - Dependencies: json (æ¨™æº–åº«ï¼Œç„¡éœ€ç‰¹åˆ¥è™•ç†)
    # - Error Handling: ä½¿ç”¨ Databricks Workflows çš„ retry policy
    # - Trace: MLflow è¨˜éŒ„é…ç½®å…§å®¹å’Œè¼‰å…¥æ™‚é–“
    """
    print(f"[Fetch] è®€å–æ•¸æ“šé…ç½®...")
    
    profile_path = PROJECT_ROOT / "data_fetch_profile.json"
    
    if not profile_path.exists():
        print(f"[Fetch] æœªæ‰¾åˆ° data_fetch_profile.jsonï¼Œå˜—è©¦ç”Ÿæˆ...")
        
        # Recovery: å¦‚æœ pandas ä¸å¯ç”¨ï¼Œä½¿ç”¨åŸºæœ¬æª”æ¡ˆæª¢æŸ¥
        if not FETCH_SKILL_AVAILABLE:
            error_msg = "data-fetch skill ä¸å¯ç”¨ï¼Œä¸”ç„¡ç¾æœ‰é…ç½®"
            print(f"[Fetch] âŒ {error_msg}")
            return {
                "config": None,
                "error": error_msg
            }
        
        # å˜—è©¦ä½¿ç”¨ data-fetch skill
        try:
            task = state["task"]
            profile = load_and_profile(task)
            config = {"profile": profile, "source": "generated"}
            print(f"[Fetch] âœ… ç”Ÿæˆæ–°çš„ profile")
        except Exception as e:
            error_msg = f"ç”Ÿæˆ profile å¤±æ•—: {e}"
            print(f"[Fetch] âŒ {error_msg}")
            return {
                "config": None,
                "error": error_msg
            }
    else:
        # è®€å–ç¾æœ‰é…ç½®
        try:
            with open(profile_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            print(f"[Fetch] âœ… è®€å–ç¾æœ‰é…ç½®æˆåŠŸ")
        except Exception as e:
            error_msg = f"è®€å–é…ç½®å¤±æ•—: {e}"
            print(f"[Fetch] âŒ {error_msg}")
            return {
                "config": None,
                "error": error_msg
            }
    
    return {
        "config": config,
        "error": None
    }


def profile_node(state: FlowState) -> dict:
    """
    Phase: Profile - è¼‰å…¥ä¸¦æª¢æŸ¥ CSV æ•¸æ“š
    
    Steps:
    - è¼‰å…¥ä¸¦æª¢æŸ¥ CSV æ•¸æ“š
    
    # ğŸ¢ Databricks Migration Notes:
    # - æ­¤ node å¯å°è£ç‚º UC Function: `uc.data.profile_dataset(dataset_name: str) -> dict`
    # - Data Source: ä½¿ç”¨ Spark DataFrames è®€å– Delta Tables
    # - Statistics: ä½¿ç”¨ Spark SQL çš„ DESCRIBEã€ANALYZE TABLE æŒ‡ä»¤
    # - Performance: å°å¤§æ•¸æ“šé›†ä½¿ç”¨å–æ¨£åˆ†æ (TABLESAMPLE)
    # - Trace: MLflow è¨˜éŒ„ profile çµ±è¨ˆè³‡è¨Šå’ŒåŸ·è¡Œæ™‚é–“
    """
    config = state.get("config")
    
    print(f"[Profile] åˆ†ææ•¸æ“š profile...")
    
    if not config:
        print(f"[Profile] âš ï¸  ç„¡é…ç½®å¯åˆ†æï¼Œè·³é")
        return {"profile": None}
    
    # å¾é…ç½®ä¸­æå– profile è³‡è¨Š
    if "profile" in config:
        profile = config["profile"]
    elif "dataset" in config:
        profile = config
    else:
        profile = config
    
    print(f"[Profile] âœ… Profile è³‡è¨Š:")
    if isinstance(profile, dict):
        for key in ["dataset", "rows", "cols", "columns"]:
            if key in profile:
                print(f"  - {key}: {profile[key]}")
    
    return {
        "profile": profile,
        "error": None
    }


def quality_check_node(state: FlowState) -> dict:
    """
    Phase: QualityCheck - é©—è­‰æ•¸æ“šå®Œæ•´æ€§
    
    Steps:
    - é©—è­‰æ•¸æ“šå®Œæ•´æ€§
    
    # ğŸ¢ Databricks Migration Notes:
    # - æ­¤ node å¯å°è£ç‚º UC Function: `uc.data.quality_check(profile: dict) -> dict`
    # - Integration: æ•´åˆ Databricks Lakehouse Monitoring é€²è¡Œè‡ªå‹•å“è³ªç›£æ§
    # - Metrics: å®šç¾©å“è³ª SLA (null_ratio < 5%, completeness > 95%)
    # - Alerts: å“è³ªä¸é”æ¨™æ™‚é€é Databricks Alerts é€šçŸ¥
    # - Trace: MLflow è¨˜éŒ„å“è³ªæŒ‡æ¨™å’Œé–¾å€¼æ¯”è¼ƒçµæœ
    """
    profile = state.get("profile")
    
    print(f"[QualityCheck] æª¢æŸ¥æ•¸æ“šå“è³ª...")
    
    if not profile:
        return {
            "quality_report": {
                "status": "skipped",
                "reason": "no profile available"
            }
        }
    
    # åŸ·è¡Œå“è³ªæª¢æŸ¥
    quality_report = {
        "status": "unknown",
        "checks": []
    }
    
    # æª¢æŸ¥ 1: æ•¸æ“šåˆ—æ•¸
    if isinstance(profile, dict) and "rows" in profile:
        rows = profile["rows"]
        if rows > 0:
            quality_report["checks"].append({
                "name": "row_count",
                "status": "pass",
                "value": rows
            })
        else:
            quality_report["checks"].append({
                "name": "row_count",
                "status": "fail",
                "value": rows,
                "message": "æ•¸æ“šç‚ºç©º"
            })
    
    # æª¢æŸ¥ 2: Null å€¼æ¯”ä¾‹
    if isinstance(profile, dict) and "null_counts" in profile:
        null_counts = profile["null_counts"]
        total_nulls = sum(null_counts.values())
        rows = profile.get("rows", 0)
        cols = profile.get("cols", 0)
        
        if rows > 0 and cols > 0:
            null_ratio = total_nulls / (rows * cols)
            quality_report["checks"].append({
                "name": "null_ratio",
                "status": "pass" if null_ratio < 0.05 else "warning",
                "value": f"{null_ratio:.2%}",
                "threshold": "< 5%"
            })
    
    # åˆ¤æ–·æ•´é«”ç‹€æ…‹
    if all(check["status"] == "pass" for check in quality_report["checks"]):
        quality_report["status"] = "pass"
    elif any(check["status"] == "fail" for check in quality_report["checks"]):
        quality_report["status"] = "fail"
    else:
        quality_report["status"] = "warning"
    
    print(f"[QualityCheck] âœ… å“è³ªæª¢æŸ¥å®Œæˆ: {quality_report['status']}")
    for check in quality_report["checks"]:
        print(f"  - {check['name']}: {check['status']} ({check.get('value', 'N/A')})")
    
    return {
        "quality_report": quality_report,
        "error": None
    }


def summarize_node(state: FlowState) -> dict:
    """
    Phase: Summarize - ç”Ÿæˆæ•¸æ“šæ‘˜è¦å ±å‘Š
    
    Steps:
    - ç”Ÿæˆæ•¸æ“šæ‘˜è¦å ±å‘Š
    
    # ğŸ¢ Databricks Migration Notes:
    # - æ­¤ node å¯å°è£ç‚º UC Function: `uc.reporting.generate_summary(state: dict) -> str`
    # - Output: å°‡æ‘˜è¦å¯«å…¥ Delta Table æˆ– DBFS è€Œéæœ¬åœ°æª”æ¡ˆ
    # - Template: ä½¿ç”¨ Databricks SQL çš„ Dashboard æˆ– Markdown widgets
    # - Distribution: é€é Databricks Jobs çš„ email notification ç™¼é€å ±å‘Š
    # - Trace: MLflow è¨˜éŒ„æ‘˜è¦ç”Ÿæˆæ™‚é–“å’Œå…§å®¹é•·åº¦
    """
    profile = state.get("profile")
    quality_report = state.get("quality_report")
    
    print(f"[Summarize] ç”Ÿæˆæ•¸æ“šæ‘˜è¦...")
    
    summary_lines = ["## æ•¸æ“šåˆ†ææ‘˜è¦\n"]
    
    # ä»»å‹™è³‡è¨Š
    summary_lines.append(f"**ä»»å‹™:** {state['task']}\n")
    
    # Profile æ‘˜è¦
    if profile and isinstance(profile, dict):
        summary_lines.append("### æ•¸æ“šæ¦‚è¦½")
        if "dataset" in profile:
            summary_lines.append(f"- æ•¸æ“šé›†: {profile['dataset']}")
        if "rows" in profile:
            summary_lines.append(f"- ç¸½åˆ—æ•¸: {profile['rows']:,}")
        if "cols" in profile:
            summary_lines.append(f"- ç¸½æ¬„ä½æ•¸: {profile['cols']}")
        if "columns" in profile:
            summary_lines.append(f"- æ¬„ä½åç¨±: {', '.join(profile['columns'])}")
        summary_lines.append("")
    
    # å“è³ªå ±å‘Šæ‘˜è¦
    if quality_report and isinstance(quality_report, dict):
        summary_lines.append("### å“è³ªæª¢æŸ¥")
        summary_lines.append(f"- æ•´é«”ç‹€æ…‹: {quality_report.get('status', 'unknown').upper()}")
        
        if "checks" in quality_report:
            for check in quality_report["checks"]:
                status_emoji = "âœ…" if check["status"] == "pass" else "âš ï¸" if check["status"] == "warning" else "âŒ"
                summary_lines.append(f"- {check['name']}: {status_emoji} {check.get('value', 'N/A')}")
        summary_lines.append("")
    
    # éŒ¯èª¤è³‡è¨Š
    if state.get("error"):
        summary_lines.append("### âš ï¸ éŒ¯èª¤")
        summary_lines.append(f"```\n{state['error']}\n```\n")
    
    summary = "\n".join(summary_lines)
    
    print(f"[Summarize] âœ… æ‘˜è¦ç”Ÿæˆå®Œæˆ ({len(summary)} å­—å…ƒ)")
    
    return {
        "summary": summary,
        "error": None
    }


# =============================================================================
# Graph Construction
# =============================================================================

def build_graph() -> StateGraph:
    """
    æ§‹å»º LangGraph æµç¨‹åœ–
    
    # ğŸ¢ Databricks Migration Notes:
    # - Graph Structure: å¯è½‰æ›ç‚º Databricks Workflows çš„ Task Dependencies
    # - Parallel Execution: è­˜åˆ¥å¯å¹³è¡ŒåŸ·è¡Œçš„ nodesï¼ˆç›®å‰ç‚ºç·šæ€§æµç¨‹ï¼‰
    # - Conditional Routing: æ ¹æ“š error ç‹€æ…‹æ±ºå®šæ˜¯å¦è·³éå¾ŒçºŒ nodes
    # - Retry Logic: æ¯å€‹ node å°æ‡‰ä¸€å€‹ Databricks Task çš„ retry policy
    # - Monitoring: ä½¿ç”¨ Databricks Job Runs è¿½è¹¤æ•´é«” workflow åŸ·è¡Œç‹€æ…‹
    """
    graph = StateGraph(FlowState)
    
    # åŠ å…¥æ‰€æœ‰ phase nodes
    graph.add_node("understand", understand_node)
    graph.add_node("fetch", fetch_node)
    graph.add_node("profile", profile_node)
    graph.add_node("quality_check", quality_check_node)
    graph.add_node("summarize", summarize_node)
    
    # ç·šæ€§é€£æ¥æ‰€æœ‰ phases
    graph.add_edge(START, "understand")
    graph.add_edge("understand", "fetch")
    graph.add_edge("fetch", "profile")
    graph.add_edge("profile", "quality_check")
    graph.add_edge("quality_check", "summarize")
    graph.add_edge("summarize", END)
    
    return graph.compile()


# å»ºç«‹å¯åŒ¯å‡ºçš„ graph instance
graph = build_graph()
